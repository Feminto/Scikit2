{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d014584e",
   "metadata": {},
   "source": [
    "# Pipelines in Python and scikit-learn\n",
    "\n",
    "The workflow of any machine learning project includes all the steps required to build it. A proper ML project consists of basically four main parts are given as follows: \n",
    " \n",
    "\n",
    "Gathering data: \n",
    "The process of gathering data depends on the project it can be real-time data or the data collected from various sources such as a file, database, survey and other sources.\n",
    "\n",
    "Data pre-processing: \n",
    "Usually, within the collected data, there is a lot of missing data, extremely large values, unorganized text data or noisy data and thus cannot be used directly within the model, therefore, the data require some pre-processing before entering the model.\n",
    "\n",
    "Training and testing the model: Once the data is ready for algorithm application, It is then ready to put into the machine learning model. Before that, it is important to have an idea of what model is to be used which may give a nice performance output. The data set is divided into 3 basic sections i.e. The training set, validation set and test set. The main aim is to train data in the train set, to tune the parameters using ‘validation set’ and then test the performance test set.\n",
    "\n",
    "Evaluation: \n",
    "Evaluation is a part of the model development process. It helps to find the best model that represents the data and how well the chosen model works in the future. This is done after training of model in different algorithms is done. The main motto is to conclude the evaluation and choose model accordingly again.\n",
    "\n",
    "ML Workflow in python \n",
    "The execution of the workflow is in a pipe-like manner, i.e. the output of the first steps becomes the input of the second step. Scikit-learn is a powerful tool for machine learning, provides a feature for handling such pipes under the sklearn.pipeline module called Pipeline.  \n",
    "It takes 2 important parameters, stated as follows: \n",
    "\n",
    "\n",
    "We will cover:\n",
    "- Preprocessing using StandardScaler and OneHotEncoder\n",
    "- Creating pipelines with ColumnTransformer\n",
    "- Combining preprocessing and classifiers in a single pipeline\n",
    "- Using cross-validation and GridSearchCV with pipelines\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1e92f163",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Pipeline] ............... (step 1 of 3) Processing pca, total=   0.0s\n",
      "[Pipeline] ............... (step 2 of 3) Processing std, total=   0.0s\n",
      "[Pipeline] ..... (step 3 of 3) Processing decision_tree, total=   0.0s\n",
      "0.9473684210526315\n"
     ]
    }
   ],
   "source": [
    "from sklearn import datasets\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "# import some data within sklearn for iris classification \n",
    "iris = datasets.load_iris()\n",
    "X = iris.data \n",
    "y = iris.target\n",
    "\n",
    "# Splitting data into train and testing part\n",
    "# The 25 % of data is test size of the data \n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.25)\n",
    "# importing pipes for making the Pipe flow\n",
    "from sklearn.pipeline import Pipeline\n",
    "# pipe flow is :\n",
    "# PCA(Dimension reduction to two) -> Scaling the data -> DecisionTreeClassification \n",
    "pipe = Pipeline([('pca', PCA(n_components = 2)), ('std', StandardScaler()), ('decision_tree', DecisionTreeClassifier())], verbose = True)\n",
    "\n",
    "# fitting the data in the pipe\n",
    "pipe.fit(X_train, y_train)\n",
    "\n",
    "# scoring data \n",
    "from sklearn.metrics import accuracy_score\n",
    "print(accuracy_score(y_test, pipe.predict(X_test)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c7afb3a",
   "metadata": {},
   "source": [
    "**Hyper parameters:**\\\n",
    "There are different set of hyper parameters set within the classes passed in as a pipeline. To view them, pipe.get_params() method is used. This method returns a dictionary of the parameters and descriptions of each classes in the pipeline. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "59ebcad9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Pipeline] ............... (step 1 of 3) Processing pca, total=   0.0s\n",
      "[Pipeline] ............... (step 2 of 3) Processing std, total=   0.0s\n",
      "[Pipeline] ..... (step 3 of 3) Processing Decision_tree, total=   0.0s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'memory': None,\n",
       " 'steps': [('pca', PCA(n_components=2)),\n",
       "  ('std', StandardScaler()),\n",
       "  ('Decision_tree', DecisionTreeClassifier())],\n",
       " 'verbose': True,\n",
       " 'pca': PCA(n_components=2),\n",
       " 'std': StandardScaler(),\n",
       " 'Decision_tree': DecisionTreeClassifier(),\n",
       " 'pca__copy': True,\n",
       " 'pca__iterated_power': 'auto',\n",
       " 'pca__n_components': 2,\n",
       " 'pca__n_oversamples': 10,\n",
       " 'pca__power_iteration_normalizer': 'auto',\n",
       " 'pca__random_state': None,\n",
       " 'pca__svd_solver': 'auto',\n",
       " 'pca__tol': 0.0,\n",
       " 'pca__whiten': False,\n",
       " 'std__copy': True,\n",
       " 'std__with_mean': True,\n",
       " 'std__with_std': True,\n",
       " 'Decision_tree__ccp_alpha': 0.0,\n",
       " 'Decision_tree__class_weight': None,\n",
       " 'Decision_tree__criterion': 'gini',\n",
       " 'Decision_tree__max_depth': None,\n",
       " 'Decision_tree__max_features': None,\n",
       " 'Decision_tree__max_leaf_nodes': None,\n",
       " 'Decision_tree__min_impurity_decrease': 0.0,\n",
       " 'Decision_tree__min_samples_leaf': 1,\n",
       " 'Decision_tree__min_samples_split': 2,\n",
       " 'Decision_tree__min_weight_fraction_leaf': 0.0,\n",
       " 'Decision_tree__random_state': None,\n",
       " 'Decision_tree__splitter': 'best'}"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn import datasets\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "# import some data within sklearn for iris classification \n",
    "iris = datasets.load_iris()\n",
    "X = iris.data \n",
    "y = iris.target\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.25)\n",
    "\n",
    "from sklearn.pipeline import Pipeline\n",
    "pipe = Pipeline([('pca', PCA(n_components = 2)), ('std', StandardScaler()), ('Decision_tree', DecisionTreeClassifier())], verbose = True)\n",
    "\n",
    "pipe.fit(X_train, y_train)\n",
    "\n",
    "# to see all the hyper parameters\n",
    "pipe.get_params()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
